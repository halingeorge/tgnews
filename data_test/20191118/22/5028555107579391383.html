<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8"/>
    <meta property="og:url" content="https://arstechnica.com/science/2019/11/giving-autonomous-cars-a-theory-of-mind-improves-their-integration/"/>
    <meta property="og:site_name" content="Ars Technica"/>
    <meta property="article:published_time" content="2019-11-18T22:59:34+00:00"/>
    <meta property="og:title" content="Improving autonomous autos by having them guess which humans are selfish"/>
    <meta property="og:description" content="Estimating whether fellow drivers are selfish or altruistic makes a big difference."/>
  </head>
  <body>
    <article>
      <h1>Improving autonomous autos by having them guess which humans are selfish</h1>
      <h2>Estimating whether fellow drivers are selfish or altruistic makes a big difference.</h2>
      <address><time datetime="2019-11-18T22:59:34+00:00">18 Nov 2019, 22:59</time> by <a rel="author">John Timmer</a></address>
      <p>Imagine you're trying to make a left turn onto a busy road. Car after car rolls past, keeping you trapped as your frustration rises. Finally, a generous driver decelerates enough to create a gap. A check of the traffic from the opposite direction, a quick bit of acceleration, and you're successfully merged into traffic.</p>
      <p>This same scene plays out across the world countless times a day. And it's a situation where inferring both the physics and the motives of your fellow drivers is difficult, as evidenced by the fact that the United States sees 1.4 million accidents each year from drivers in the process of turning. Now imagine throwing autonomous vehicles into the mix. These are typically limited to evaluating only the physics and to make conservative decisions in situations where information is ambiguous.</p>
      <p>Now, a group of computer scientists has figured out how to improve autonomous vehicle (AV) performance in these circumstances. The scientists have essentially given their AVs a limited theory of mind, allowing the vehicles to better interpret what the behaviors of their nearby human drives are telling them.</p>
      <h3>Mind the theory</h3>
      <p><a href="https://arstechnica.com/science/2019/11/great-apes-pass-a-false-belief-test-hinting-at-a-theory-of-mind/">Theory of mind</a> comes so easily to us that it's difficult to recognize how rare it is outside of our species. We're easily able to recognize that our fellow humans have minds like our own, and we use that recognition to infer things like the state of their knowledge and their likely motivations. These inferences are essential to most of our social activities, driving included. While a friendly wave can make for an unambiguous signal that your fellow driver is offering you space in their lane, we can often make inferences based simply on the behavior of their car.</p>
      <p>And, critically, autonomous vehicles aren't especially good at this. In many cases, their own behavior doesn't send signals back to other drivers. A study of accidents involving AVs in California indicated that over half of them involved the AV being rear-ended because a human driver couldn't figure out what in the world it was doing. (Volvo, among others, is <a href="https://arstechnica.com/cars/2018/09/volvo-gets-optimistic-about-the-future-with-the-360c-driverless-concept/">working to change that</a>.)</p>
      <p>It's unrealistic to think that we'll give AVs a full-blown theory of mind any time soon. AIs are simply not that advanced, and it would be excessive for cars, which only have to deal with a limited range of human behaviors. But a group of researchers at MIT and Delft University of Technology has decided that putting an extremely limited theory of mind in place for certain driving decisions, including turns and merges, should be possible.</p>
      <p>The idea behind the researchers' work, described in a new paper in PNAS, involves a concept called <a href="https://en.wikipedia.org/wiki/Social_value_orientations">social value orientation</a>, which is a way of measuring how selfish or community-oriented an individual's actions are. While there are undoubtedly detailed surveys that can provide a meticulous description of a person's social value orientation, autonomous vehicles generally won't have the time to be giving their fellow drivers surveys.</p>
      <p>So the researchers distilled social value orientation into four categories: altruists, who try to maximize the enjoyment of their fellow drivers; prosocial drivers, who try to take actions that allow all other drivers to benefit (which may occasionally involve selfishly flooring it); individualists, who maximize their own driving experience; and competitive drivers, who only care about having a better driving experience than those around them.</p>
      <h3>Value-oriented</h3>
      <p>The researchers developed a formula that would let them calculate the expected driving trajectory for each of these categories given the starting position of other cars. The autonomous vehicle was programmed to compare the trajectories of actual drivers to the calculated version and use that to determine which of the four categories the drivers were likely to be in. Given that classification, the vehicle could then project what their future actions would be. As the researchers wrote, "we extend the ability of AVs' reasoning by incorporating estimates of the other drivers' personality and driving style from social cues."</p>
      <p>This is substantially different from some game-theory work that's been done in the area. That work has assumed that every driver is always maximizing their own gain; if altruism emerges, it's only incidental to this maximization. This new work, in contrast, bakes altruistic behavior into its calculations and recognizes that drivers are complicated and may change their tendencies as situations evolve. In fact, previous studies had indicated that in contexts other than driving, about half of the people tested showed prosocial behavior, with another 40% being selfish.</p>
      <p>With the system in place, the researchers obtained data on vehicle locations and trajectories as drivers merged onto a highway, a situation that often requires the generosity of fellow drivers. With the social value orientation system in place, the autonomous driver was able make more accurate predictions of its fellow drivers' trajectories than it could without—prediction errors dropped by 25%. The system also worked on lane changes on crowded freeways, as well as turns into traffic.</p>
      <p>Using these evaluations, the researchers could also make some inferences using the traffic patterns they had. For example, they found that a highway driver may start out selfishly following the car in front of them, shift to altruistic as they decelerate to allow a driver to merge, then switch right back to a selfish approach. Similarly, drivers facing a merge onto a freeway typically ended up being competitive—something you see every time a vehicle pulls out and slows down everyone who was stuck in the lane behind it.</p>
      <p>While we're still a long way off from giving autonomous vehicles a general AI or a full theory of mind, the research shows that you can get significant benefits from giving AVs a very limited one. And it's a nice demonstration that if we want any autonomous system to integrate with something that's currently a social activity, then paying attention to what social scientists have figured out about those activities can be incredibly valuable.</p>
      <p>PNAS, 2019. DOI: <a href="http://dx.doi.org/10.1073/pnas.1820676116">10.1073/pnas.1820676116</a>  (<a href="http://arstechnica.com/science/news/2010/03/dois-and-their-discontents-1.ars">About DOIs</a>).</p>
      <related>
        <h4>Further Reading</h4>
        <a href="https://arstechnica.com/science/2019/11/great-apes-pass-a-false-belief-test-hinting-at-a-theory-of-mind/"/>
      </related>
    </article>
  </body>
</html>